<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Xiangming Gu</title>
  
  <meta name="author" content="Xiangming Gu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Xiangming Gu</name>
              </p>
              <p>
                I am a final-year Ph.D. candidate from <a href="https://www.nus.edu.sg">National University of Singapore</a>. I finished my bachelor degrees from <a href="https://www.tsinghua.edu.cn/en/">Tsinghua University</a> in 2021. I was a student researcher at <a href="https://deepmind.google/">Google Deepmind</a> and a research intern at <a href="https://sail.sea.com">Sea AI Lab</a>.
                <!-- I am a final-year Ph.D. candidate from <a href="https://www.nus.edu.sg">National University of Singapore</a>, supervised by Prof. <a href="https://www.comp.nus.edu.sg/cs/people/wangye/">Ye Wang</a>. I earned my B.E. degree of Electronic Engineering and B.S. degree of Finance from <a href="https://www.tsinghua.edu.cn/en/">Tsinghua University</a> in 2021. -->
              </p>
              <!-- <p>
                I am a student researcher at <a href="https://deepmind.google/">Google Deepmind</a> in Singapore (was based in London, United Kingdom), where I am hosted by <a href="https://petar-v.com/">Petar Veliƒçkoviƒá</a> and <a href="https://scholar.google.ru/citations?user=jM6Y0yAAAAAJ&hl=en">Larisa Markeeva</a>, and work closely with <a href="https://www.razp.info">Razvan Pascanu</a> and <a href="https://sohamde.github.io">Soham De</a>. I was lucky to be mentored by <a href="https://p2333.github.io">Tianyu Pang</a> and <a href="https://duchao0726.github.io">Chao Du</a> at <a href="https://sail.sea.com">Sea AI lab (SAIL)</a>, Singapore, where I also worked closely with <a href="https://siviltaram.github.io">Qian Liu</a> and <a href="https://linmin.me">Min Lin</a>.
              </p> -->
              <p>
                My recent research focus is to <strong>understand, advance and safely deploy generative models and agents</strong>. My next vision is (i) what's the next generation of thinking paradigm to enable LLMs solve challenging questions, e.g., scientific discovery? (ii) what's the next generation of scaling axis in LLM architecture?
              </p>
              <!-- <p>
                <font color="red"><strong>I am looking for full-time positions of research scientist or member of technical staff, please contact me if you are interested in my research.</strong></font>
              </p> -->

              <p style="text-align:center">
                <a href="mailto:xiangming@u.nus.edu">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=BkxEuIoAAAAJ&hl">Google Scholar</a> &nbsp/&nbsp
                <a href="https://openreview.net/profile?id=~Xiangming_Gu1">Openreview</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/xiangming-gu/">Linkedin</a> &nbsp/&nbsp
                <a href="https://twitter.com/gu_xiangming">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/guxm2021">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/XiangmingGu.JPG"><img style="width:60%;max-width:60%" alt="profile photo" src="images/XiangmingGu.JPG" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        
        <HR>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:10px;width:100%;vertical-align:middle">
            <heading>News</heading>

          <ul>
            <li>
              <strong>[2026.01]</strong>: I released <a href="https://github.com/google-deepmind/gemma_penzai">gemma_penzai</a>, a JAX package to look into LLM internals and debug LLMs with multi-modal support. This work was one of my projects at Google Deepmind. See the <a href="https://github.com/google-deepmind/gemma_penzai/tree/main/notebooks">tutorials</a> and <a href="https://x.com/gu_xiangming/status/2012886825019609294?s=20">post</a>.
            </li>
            <li>
              <strong>[2026.01]</strong>: I was invited to give a talk about <a href="./pdf/attention_sink_ns_talk.pdf">Demystifying Attention Sink in LLMs and its Applications to Architecture Design</a> by <a href="https://aerlabs.tech">AER Labs</a> at <a href="https://ns.com">Network School</a>.
            </li>
            <li>
              <strong>[2025.11]</strong>: I was invited to give a talk about <a href="pdf/attention_sink_hunyuan_talk.pdf">Attention Sink in LLMs and its Applications</a> by <a href="https://www.ee.tsinghua.edu.cn/en/">Department of Electronic Engineering, Tsinghua University</a> and <a href="https://hunyuan.tencent.com/visual">Tencent Hunyuan</a>.
            </li>
            <!-- <li>
              <strong>[2025.11]</strong>: I re-joined <a href="https://deepmind.google/">Google Deepmind (GDM)</a> in Singapore office to continue as a student researcher!
            </li> -->
            <li>
              <strong>[2025.10]</strong>: I was glad to give a final presentation to wrap up my student researcher at Google Deepmind: <strong>Looking into LLMs: From Tokens to Solutions</strong>.
            </li>
            <li>
              <strong>[2025.10]</strong>: We released a technical report titled <a href="https://arxiv.org/pdf/2510.18554">Extracting Alignment Data in Open Models</a>!
            </li>
            <li>
              <strong>[2025.09]</strong>: One paper got accepted to Advances in Neural Information Processing Systems (<i>NeurIPS</i>'2025)!
            </li>
            <li>
              <strong>[2025.07]</strong>: One paper got accepted to Conference on Language Modeling (<i>COLM</i>'2025)!
            </li>
            <li>
              <strong>[2025.06]</strong>: I was glad to give a talk about <strong>Understanding Attention Sink in (Large) Language Models</strong> in the team of Deep Learning: Agent Frontier at Google Deepmind.
            </li>
            <li>
              <strong>[2025.05]</strong>: I was invited to give a talk on <a href="pdf/ASAP_seminar_attention_sink.pdf">When Attention Sink Emerges in Language Models: An Empirical View</a> by  <a href="https://asap-seminar.github.io">ASAP Seminar Series</a>!
            </li>
            <li>
              <strong>[2025.05]</strong>: I joined <a href="https://deepmind.google/">Google Deepmind (GDM)</a> as a student researcher in London, United Kingdom!
            </li>
            <!-- <li>
              <strong>[2025.03]</strong>: I passed my Ph.D. thesis proposal review!
            </li> -->
            <li>
              <strong>[2025.02]</strong>: I was invited to give a talk titled <a href="pdf/Research_week_talk.pdf">On the Interpretability and Safety of Generative Models</a> by <a href="https://researchweek.comp.nus.edu.sg/computing/">research week open house</a> of NUS!
            </li>
            <li>
              <strong>[2025.01]</strong>: Two papers with one spotlight and one poster got accepted to International Conference on Learning Representations (<i>ICLR</i>'2025), one paper got accepted to Transactions on Machine Learning Research (<i>TMLR</i>'2025)!
            </li>
            <li>
              <strong>[2025.01]</strong>: I gave a <a href="pdf/gyss_agent_smith_poster.pdf">poster presentation</a> about Agent Smith during <a href="https://gyss.nrf.gov.sg">Global Young Scientists Summit 2025</a>!
            </li>
            <!-- <li>
              <strong>[2024.10]</strong>: I was selected as a participant for <a href="https://gyss.nrf.gov.sg">Global Young Scientists Summit 2025</a>!
            </li>
            <li>
              <strong>[2024.07]</strong>: I received Dean's Graduate Research Excellence Award from School of Computing, NUS!
            </li>
            <li>
              <strong>[2024.05]</strong>: One paper got accepted to International Conference on Machine Learning (<i>ICML</i>'2024)!
            </li> -->
            <!-- <li>
              <strong>[2024.02]</strong>: One paper got accepted to ACM Transactions on Multimedia Computing Communications and Applications (<i>TOMM</i>'2024)!
            </li> -->
            <!-- <li>
              <strong>[2024.02]</strong>: We released <a href="https://arxiv.org/abs/2402.08567">Agent Smith</a>, which got posted as "Here Come the AI Worms" on <a href="https://www.wired.com/story/here-come-the-ai-worms/?utm_social-type=owned&utm_source=linkedin&utm_medium=social&utm_campaign=wired&utm_brand=wired&mbid=social_linkedin">WIRED Magazine</a>!
          </li> -->
            <!-- <li>
              <strong>[2023.09]</strong>: One paper got accepted to IEEE Transactions on Audio, Speech and Language Processing (<i>TASLP</i>'2023)!
          </li>
            <li>
              <strong>[2023.07]</strong>: One paper got accepted to ACM International Conference on Multimedia (<i>MM</i>'2023)!
          </li> -->
            <!-- <li>
              <strong>[2023.03]</strong>: I joined <a href="https://sail.sea.com">Sea AI lab (SAIL)</a> as a research intern and my research is related to generative models and (multimodal) large language models!
          </li> -->
            <!-- <li>
              <strong>[2023.03]</strong>: I was invited to be a reviewer for International Conference on Computer Vision (<i>ICCV</i>'2023)!
          </li> -->
            <!-- <li>
              <strong>[2023.01]</strong>: I received Research Achievement Award from School of Computing, NUS!
          </li>
          <li>
              <strong>[2022.12]</strong>: One paper got accepted to Transactions on Machine Learning Research (<i>TMLR</i>'2022)!
          </li>
          <li>
              <strong>[2022.12]</strong>: I passed my Ph.D. Qualifying Examination (PQE) and became a Ph.D. candidate!
          </li> -->
          <!-- <li>
            <strong>[2022.11]</strong>: I physically attended <i>NeurIPS</i>'2022 in New Orleans, United States!
          </li>
          <li>
            <strong>[2022.10]</strong>: I physically attended <i>MM</i>'2022 in Lisbon, Portugal, where our paper <a href="https://dl.acm.org/doi/10.1145/3503161.3548411">MM-ALT</a> won the <strong>Top Paper Award</strong>!
          </li> -->
            <!-- <li>
              <strong>[2022.09]</strong>: One paper got accepted to Advances in Neural Information Processing Systems (<i>NeurIPS</i>'2022)!
          </li>
            <li>
              <strong>[2022.07]</strong>: One paper got accepted to International Society for Music Information Retrieval Conference (<i>ISMIR</i>'2022)!
          </li>
            <li>
                <strong>[2022.06]</strong>: One paper got accepted to ACM International Conference on Multimedia (<i>MM</i>'2022) as oral presentation, which also won the <strong>Top Paper Award</strong>!
            </li>
            <li>
              <strong>[2022.05]</strong>: One paper got accepted to IEEE Transactions on Image Processing (<i>TIP</i>'2022)!
          </li> -->
          </ul>
          </td>

        </tr>
      </tbody></table>

      <HR>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:10px;width:100%;vertical-align:middle">
            <heading>Selected Research</heading>
          </td>
        </tr>
      </tbody></table>
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <!-- <table width="100%" align="center" border="0" cellpadding="10"><tbody> -->
              
          <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              * denotes equal contribution. Please see my
              <a href="https://scholar.google.com/citations?user=BkxEuIoAAAAJ&hl">Google Scholar</a> for full list. 
              <!-- My representative papers are <span class="highlight">highlighted</span>. -->
            </td>
          </tr>
          

        <!-- </tbody></table> -->

        <tr>
          <td style="padding:10px;width:100%;vertical-align:middle">
            <u>LLMs Reasoning</u>
          </td>
  
        </tr>

        <tr>
          <td style="padding:10px;width:100%;vertical-align:middle">
              <papertitle>Parallel and Sequential Test-Time-Scaling in Large Reasoning Models</papertitle>
            </a>
            <br>
            <u><strong>Xiangming Gu</strong></u> and the Team
            <br>
              Google Deepmind Internal Technical Report, 2025.
            <br>
          </td>
        </tr>
      
        <tr>
          <td style="padding:10px;width:100%;vertical-align:middle">
            <u>LLMs Pre-training and Attention</u>
          </td>
  
        </tr>

        <!-- <tr style="background-color: #ffffd0;"> -->
        <tr>
          <td style="padding:10px;width:100%;vertical-align:middle">
            <a href="https://openreview.net/forum?id=78Nn4QJTEN">
              <papertitle>When Attention Sink Emerges in Language Models: An Empirical View</papertitle>
            </a>
            <br>
            <u><strong>Xiangming Gu</strong></u>,
            <a href="https://p2333.github.io">Tianyu Pang</a>,
            <a href="https://duchao0726.github.io">Chao Du</a>,
            <a href="https://siviltaram.github.io">Qian Liu</a>,
            <a href="https://scholar.google.com/citations?user=qLpVG2IAAAAJ&hl">Fengzhuo Zhang</a>,
            <a href="https://scholar.google.com/citations?user=4gFE1iYAAAAJ&hl">Cunxiao Du</a>,
            <a href="https://www.comp.nus.edu.sg/cs/people/wangye/">Ye Wang</a>,
            <a href="http://linmin.me">Min Lin</a>
            <br>
            <!-- <strong>Highlights</strong>: (i) We presented mechanism understanding of attention sink, massive activations, and value drains in LLMs. (ii) We answered when attention sink emerges in LLMs from pre-training perspective. (iii) We represented the early explorations on LLM architecture designs from the perspective of attention sink biases, and eliminating attention sink. (iv) This research can explain the motivations of attention biases used in large-scale LLMs, like <a href="https://arxiv.org/abs/2508.10925">GPT-OSS</a> (OpenAI), and <a href="https://mimo.xiaomi.com/blog/mimo-v2-flash">MiMo-V2-Flash</a> (Xiaomi).
            <br> <strong>Published</strong> in  -->
              International Conference on Learning Representations (<strong>ICLR</strong>), Singapore, Singapore, 2025. (<font color="red"><strong>Spotlight</strong></font>)
            <br>
            Also in Annual Conference on Neural Information Processing Systems Workshop on Attributing Model Behavior at Scale (<strong>ATTRIB @ NeurIPS</strong>), Vancouver, Canada, 2024. (<font color="red"><strong>Oral</strong></font>)
            <br>
            <a href="https://arxiv.org/pdf/2410.10781">pdf</a> /
            <a href="https://github.com/sail-sg/Attention-Sink">code</a> /
            <a href="https://recorder-v3.slideslive.com/?share=98601&s=8d817838-dc1b-4cd7-aa9a-a6ab678811ef">video</a> /
            <a href="https://youtu.be/nII3utiA06g">long talk</a> /
            <a href="pdf/attention_sink_hunyuan_talk.pdf">slides</a> /
            <a href="pdf/ICLR2025_poster.pdf">poster</a> /
            <a href="https://x.com/gu_xiangming/status/1952811057673642227?s=20">post1</a> /
            <a href="https://x.com/gu_xiangming/status/1993896237083578741?s=20">post2</a> /
            <a href="https://x.com/gu_xiangming/status/1995151989190303972?s=20">post3</a>
          </td>
        </tr>

          <!-- <tr style="background-color: #ffffd0;"> -->
          <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2504.02732">
                <papertitle>Why Do LLMs Attend to the First Token?</papertitle>
              </a>
              <br>
              <a href="https://federicobarbero.com">Federico Barbero</a>*,
              <a href="https://scholar.google.co.uk/citations?user=P1qHzNYAAAAJ&hl=en">√Ålvaro Arroyo</a>*,
              <u><strong>Xiangming Gu</strong></u>,
              <a href="https://cperivol.com">Christos Perivolaropoulos</a>,
              <a href="https://www.cs.ox.ac.uk/people/michael.bronstein/">Michael Bronstein</a>,
              <a href="https://petar-v.com/">Petar Veliƒçkoviƒá</a>,
              <a href="https://sites.google.com/view/razp/home">Razvan Pascanu</a>
              <br>
              <!-- <strong>Highlights</strong>: (i) We empirically and theoretically showed that LLMs need "no-op" to avoid over-mixing, especially in long-context scenario. (ii) We demonstrated that attention sink is one way to approximate "no-op". (iii) This research can explain the motivations of gated attention used in large-scale LLMs, like <a href="https://qwen.ai/blog?id=e34c4305036ce60d55a0791b170337c2b70ae51d&from=home.latest-research-list">Qwen3-Next</a> (Alibaba).
              <br> <strong>Published</strong> in  -->
                Conference on Language Modeling (<strong>COLM</strong>), Montreal, Canada, 2025.
              <br>
              <a href="https://arxiv.org/pdf/2504.02732">pdf</a> /
              <a href="pdf/attention_sink_hunyuan_talk.pdf">slides</a>
              <!-- <p></p> -->
            </td>
          </tr>
         

          <!-- <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2503.15450">
                <papertitle>SkyLadder: Better and Faster Pretraining via Context Window Scheduling</papertitle>
              </a>
              <br>
              <a href="https://tongyao-zhu.github.io/">Tongyao Zhu</a>,
              <a href="https://siviltaram.github.io">Qian Liu</a>,
              <a href="https://charles-haonan-wang.me/">Haonan Wang</a>,
              <a href="https://shiqichen17.github.io/">Shiqi Chen</a>,
              <u><strong>Xiangming Gu</strong></u>,
              <a href="https://p2333.github.io">Tianyu Pang</a>,
              <a href="https://www.comp.nus.edu.sg/~kanmy/">Min-Yen Kan</a>
              <br>
               Annual Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>), San Diego, USA, 2025.
              <br>
                Also in International Conference on Learning Representations Workshop on Open Science for Foundation Models (<strong>SCI-FM @ ICLR</strong>), Singapore, Singapore, 2025.
              <br>
              <a href="https://arxiv.org/pdf/2503.15450">pdf</a> /
              <a href="https://github.com/sail-sg/SkyLadder">code</a>
            </td>
          </tr> -->
          

          <!-- <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <u>Advancing Generative Models</u>
            </td>
    
          </tr> -->

          <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <u>Memorization, Generalization, and Safety</u>
            </td>
    
          </tr>

          <!-- <tr style="background-color: #ffffd0;"> -->
          <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2510.18554">
                <papertitle>Extracting Alignment Data in Open Models</papertitle>
              </a>
              <br>
              <a href="https://federicobarbero.com">Federico Barbero</a>,
              <u><strong>Xiangming Gu</strong></u>,
              <a href="https://www.christopherchoquette.com">Christopher A. Choquette-Choo</a>,
              <a href="https://chawins.github.io">Chawin Sitawarin</a>,
              <a href="https://jagielski.github.io">Matthew Jagielski</a>,
              <a href="https://scholar.google.com/citations?user=1FmcH_QAAAAJ&hl=en">Itay Yona</a>,
              <a href="https://petar-v.com/">Petar Veliƒçkoviƒá</a>,
              <a href="https://iliaishacked.github.io">Ilia Shumailov</a>,
              <a href="https://jamiehay.es">Jamie Hayes</a>
              <br>
              <!-- <strong>Highlights</strong>: (i) We showed that with only chat template as input, alignment (post-trained) data can be extracted from post-trained (either SFT or RLVR) LLMs. (ii) We presented "di-steal-ation": the extracted data can be used to train (either SFT or RLVR) a base model, recovering a meaningful amount of the original performance.
              <br> <strong>Released</strong> as  -->
                Technical Report, 2025.
              <br>
              <a href="https://arxiv.org/pdf/2510.18554">pdf</a>
            </td>
          </tr>

          <!-- <tr style="background-color: #ffffd0;"> -->
          <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <a href="https://openreview.net/forum?id=DYMj03Gbri">
                <papertitle>Agent Smith: A Single Image Can Jailbreak <i>One Million</i> Multimodal LLM Agents Exponentially Fast</papertitle>
              </a>
              <br>
              <u><strong>Xiangming Gu</strong></u>*,
              <a href="https://xszheng2020.github.io">Xiaosen Zheng</a>*,
              <a href="https://p2333.github.io">Tianyu Pang</a>*,
              <a href="https://duchao0726.github.io">Chao Du</a>,
              <a href="https://siviltaram.github.io">Qian Liu</a>,
              <a href="https://www.comp.nus.edu.sg/cs/people/wangye/">Ye Wang</a>,
              <a href="http://www.mysmu.edu/faculty/jingjiang/">Jing Jiang</a>,
              <a href="http://linmin.me">Min Lin</a>
              <br>
              <!-- <strong>Highlights</strong>: (i) We presented theoretical framework of infectious jailbreak (or AI virus), which could jailbreak large-scale LLM-based multi-agents exponentially fast. (ii) We empirically validated the success of infectious jailbreak in LLM-based multi-agents (up to 1 million agents).
              <br>  <strong>Published</strong> in -->
              International Conference on Machine Learning (<strong>ICML</strong>), Vienna, Austria, 2024.
              <br>
              Also in International Conference on Learning Representations Workshop on Large Language Model Agents (<strong>LLMAgents @  ICLR</strong>), Vienna, Austria, 2024.
              <br>
              <a href="pdf/ICML2024.pdf">pdf</a> /
              <a href="https://sail-sg.github.io/Agent-Smith/">project page</a> /
              <a href="https://github.com/sail-sg/Agent-Smith">code</a> /
              <a href="https://youtu.be/_UHudWD9lmY">video</a> /
              <a href="pdf/ICML2024_slides.pdf">slides</a> /
              <a href="pdf/ICML2024_poster.pdf">ICML poster</a> /
              <a href="pdf/gyss_agent_smith_poster.pdf">GYSS poster</a> /
              <a href="https://www.wired.com/story/here-come-the-ai-worms/?utm_social-type=owned&utm_source=linkedin&utm_medium=social&utm_campaign=wired&utm_brand=wired&mbid=social_linkedin">WIRED press</a>
              <!-- <p>We present <i>infectious jailbreak</i>, which entails the adversary simply jailbreaking a single agent, and without any further intervention from the adversary, (almost) all agents will become infected exponentially fast and exhibit harmful behaviors. Our results show that feeding an (infectious) adversarial image into the memory of any randomly chosen agent is sufficient to achieve infectious jailbreak in multi-agent environments containing up to <i>one million</i> LLaVA-1.5 agents. Finally, we derive a simple principle for determining whether a defense mechanism can provably restrain the spread of infectious jailbreak.</p> -->
              <!-- <p></p> -->
            </td>
          </tr>


          <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <a href="https://openreview.net/forum?id=D3DBqvSDbj">
                <papertitle>On Memorization in Diffusion Models</papertitle>
              </a>
              <br>
              <u><strong>Xiangming Gu</strong></u>,
              <a href="https://duchao0726.github.io">Chao Du</a>,
              <a href="https://p2333.github.io">Tianyu Pang</a>,
              <a href="https://zhenxuan00.github.io">Chongxuan Li</a>,
              <a href="http://linmin.me">Min Lin</a>,
              <a href="https://www.comp.nus.edu.sg/cs/people/wangye/">Ye Wang</a>
              <!-- <br>
              <strong>Highlights</strong>: (i) We showed that diffusion models have theoretical optimal solutions, which can only memorize training data. (ii) We empirically explored how training recipes affect memorization of diffusion models. <strong>Published</strong> in  -->
              <br> 
              Transactions on Machine Learning Research (<strong>TMLR</strong>), 2025.
              <br>
              <a href="https://arxiv.org/pdf/2310.02664.pdf">pdf</a> /
              <a href="https://github.com/sail-sg/DiffMemorize">code</a>		
            </td>
          </tr>

          <!-- <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <a href="https://openreview.net/forum?id=wUbum0nd9N">
                <papertitle>On Calibration of LLM-based Guard Models for Reliable Content Moderation</papertitle>
              </a>
              <br>
              <a href="https://waffle-liu.github.io">Hongfu Liu</a>,
              <a href="https://sites.google.com/view/huanghengguan/homepage">Hengguan Huang</a>,
              <u><strong>Xiangming Gu</strong></u>,
              <a href="http://www.wanghao.in">Hao Wang</a>,
              <a href="https://www.comp.nus.edu.sg/cs/people/wangye/">Ye Wang</a>
              <br>
              International Conference on Learning Representations (<strong>ICLR</strong>), Singapore, Singapore, 2025.
              <br>
              Also in Annual Conference on Neural Information Processing Systems Safe Generative AI Workshop (<strong>SafeGenAI @ NeurIPS</strong>), Vancouver, Canada, 2024. (<font color="red"><strong>Oral</strong></font>)
              <br>
              <a href="https://arxiv.org/pdf/2410.10414">pdf</a> /
              <a href="https://github.com/Waffle-Liu/calibration_guard_model">code
            </td>
          </tr> -->

          <!-- <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <u>Speech/Singing and Multimodality</u>
            </td>
    
          </tr>

            <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                  <a href="https://dl.acm.org/doi/10.1145/3651310">
                    <papertitle>Automatic Lyric Transcription and Automatic Music Transcription from Multimodal Singing</papertitle>
                  </a>
                  <br>
                  <u><strong>Xiangming Gu</strong></u>,
                  <a href="https://www.oulongshen.xyz">Longshen Ou</a>,
                  <a href="https://scholar.google.com/citations?hl=zh-CN&user=Ffq7lrcAAAAJ">Wei Zeng</a>,
                  <a href="https://www.linkedin.com/in/jianan-zhang-a9014a173/?originalSubdomain=cn">Jianan Zhang</a>,
                  <a href="https://nic-wong.carrd.co">Nicholas Wong</a>,
                  <a href="https://www.comp.nus.edu.sg/cs/people/wangye/">Ye Wang</a>
                  <br>
                  ACM Transactions on Multimedia Computing Communications and Applications (<strong>TOMM</strong>), 2024.
                  <br>
                  <a href="https://dl.acm.org/doi/pdf/10.1145/3651310">pdf</a> /
                  <a href="https://github.com/guxm2021/SVT_SpeechBrain">code</a> /
                  <a href="https://zenodo.org/records/10814703">data</a>
                </td>
              </tr>

              <tr>
                  <td style="padding:10px;width:100%;vertical-align:middle">
                    <a href="https://dl.acm.org/doi/10.1145/3581783.3612272">
                      <papertitle>Elucidating Gender Fairness in Singing Voice Transcription</papertitle>
                    </a>
                    <br>
                    <u><strong>Xiangming Gu</strong></u>,
                    <a href="https://scholar.google.com/citations?hl=zh-CN&user=Ffq7lrcAAAAJ">Wei Zeng</a>,
                    <a href="https://www.comp.nus.edu.sg/cs/people/wangye/">Ye Wang</a>
                    <br>
                    ACM International Conference on Multimedia (<strong>MM</strong>), Ottawa, Canada, 2023.
                    <br>
                    <a href="pdf/ACMMM2023.pdf">pdf</a> /
                    <a href="https://github.com/guxm2021/SVT_SpeechBrain">code</a> /
                    <a href="https://youtu.be/lMQE7hQlTCQ">video</a> /
                    <a href="pdf/ACMMM2023_poster.pdf">poster</a>
                  </td>
                </tr>

                <tr>
                    <td style="padding:10px;width:100%;vertical-align:middle">
                      <a href="https://dl.acm.org/doi/10.1145/3503161.3548411">
                        <papertitle>MM-ALT: A Multimodal Automatic Lyric Transcription System</papertitle>
                      </a>
                      <br>
                      <u><strong>Xiangming Gu</strong></u>*,
                      <a href="https://www.oulongshen.xyz">Longshen Ou</a>*,
                      <a href="https://www.linkedin.com/in/danielle-ong-854b88177/">Danielle Ong</a>,
                      <a href="https://www.comp.nus.edu.sg/cs/people/wangye/">Ye Wang</a>
                      <br>
                      ACM International Conference on Multimedia (<strong>MM</strong>), Lisbon, Portugal, 2022. (<font color="red"><strong>Oral, Top Paper Award</strong></font>)
                      <br>
                      <a href="pdf/ACMMM2022.pdf">pdf</a> /
                      <a href="pdf/ACMMM2022_Appendix.pdf">appendix</a> /
                      <a href="https://n20em.github.io">project page</a> /
                      <a href="https://github.com/guxm2021/MM_ALT">code</a> /
                      <a href="https://zenodo.org/records/7545968">data</a> /
                      <a href="https://youtu.be/3eA62RYROq0">video</a> /
                      <a href="https://www.comp.nus.edu.sg/news/features/2023-marvellous-richness-wye/">press</a>
                    </td>
                  </tr>


                  <tr>
                      <td style="padding:10px;width:100%;vertical-align:middle">
                        <a href="https://arxiv.org/abs/2207.09747">
                          <papertitle>Transfer Learning of wav2vec 2.0 for Automatic Lyric Transcription</papertitle>
                        </a>
                        <br>
                        <a href="https://www.oulongshen.xyz">Longshen Ou</a>*,
                        <u><strong>Xiangming Gu</strong></u>*,
                        <a href="https://www.comp.nus.edu.sg/cs/people/wangye/">Ye Wang</a>
                        <br>
                        International Society for Music Information Retrieval Conference (<strong>ISMIR</strong>), Bengaluru, India, 2022.
                        <br>
                        <a href="pdf/ISMIR2022.pdf">pdf</a> /
                        <a href="https://github.com/guxm2021/ALT_SpeechBrain">code</a>
                      </td>
                    </tr> -->

        <!-- <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2304.12082.pdf">
                <papertitle>Deep Audio-Visual Singing Voice Transcription based on Self-Supervised Learning Models</papertitle>
              </a>
              <br>
              <strong>Xiangming Gu</strong>,
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=Ffq7lrcAAAAJ">Wei Zeng</a>,
              <a href="https://www.linkedin.com/in/jianan-zhang-a9014a173/?originalSubdomain=cn">Jianan Zhang</a>,
              <a href="https://www.oulongshen.xyz">Longshen Ou</a>,
              <a href="https://www.comp.nus.edu.sg/cs/people/wangye/">Ye Wang</a>
              <br>
              <em>Preprints</em>, 2023.
              <br>
              <a href="https://arxiv.org/pdf/2304.12082.pdf">pdf</a> /
              <a href="https://github.com/guxm2021/SVT_SpeechBrain">code</a>		
              <p></p>
              <p>We proposed the first audio-visual singing voice transcription (SVT) system to tackle the sound contamination and extend our N20EM dataset to N20EMv2. Our proposed self-supervised-learning models based SVT system demonstrates state-of-the-art performances on multiple benchmark SVT datasets.</p>
            </td>
          </tr> -->

          <!-- <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <u>Bayesian Deep Learning</u>
            </td>
    
          </tr>

          <tr>
              <td style="padding:10px;width:100%;vertical-align:middle">
                <a href="https://openreview.net/forum?id=wiHzQWwg3l">
                  <papertitle>Extrapolative Continuous-time Bayesian Neural Network for Fast Training-free Test-time Adaptation</papertitle>
                </a>
                <br>
                <a href="https://sites.google.com/view/huanghengguan/homepage">Hengguan Huang</a>,
                <u><strong>Xiangming Gu</strong></u>,
                <a href="http://www.wanghao.in">Hao Wang</a>,
                <a href="https://smcnus.github.io">Chang Xiao</a>,
                <a href="https://waffle-liu.github.io">Hongfu Liu</a>,
                <a href="https://www.comp.nus.edu.sg/cs/people/wangye/">Ye Wang</a>
                <br>
                Annual Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>), New Orleans, USA, 2022.
                <br>
                <a href="pdf/NeurIPS2022.pdf">pdf</a> /
                <a href="https://github.com/guxm2021/ECBNN">code /
                <a href="https://recorder-v3.slideslive.com/?share=76177&s=cd597422-3109-42f3-9072-dfef5c351a4d">video</a>
              </td>
            </tr>

            <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                  <a href="https://openreview.net/forum?id=29V0xo7jKp">
                    <papertitle>Unsupervised Mismatch Localization in Cross-Modal Sequential Data with Application to Mispronunciations Localization</papertitle>
                  </a>
                  <br>
                  <a href="https://scholar.google.com/citations?user=4QtVJoEAAAAJ">Wei Wei</a>*,
                  <a href="https://sites.google.com/view/huanghengguan/homepage">Hengguan Huang</a>*,
                  <u><strong>Xiangming Gu</strong></u>,
                  <a href="http://www.wanghao.in">Hao Wang</a>,
                  <a href="https://www.comp.nus.edu.sg/cs/people/wangye/">Ye Wang</a>
                  <br>
                  Transactions on Machine Learning Research (<strong>TMLR</strong>), 2022.
                  <br>
                  <a href="pdf/TMLR2022.pdf">pdf</a> /
                  <a href="https://github.com/weiwei-ww/ML-VAE">code</a>
                </td>
              </tr> -->

        </tbody></table>

        <!-- <HR>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <heading>Full Publications</heading>

            </td>

          </tr>
        </tbody></table> -->
        
<!-- 
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:10px;width:100%;vertical-align:middle">
            <heading><u>2024</u></heading>
  
          </td>
  
        </tr>
      </tbody></table> -->



      <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        

          <tr>
              <td style="padding:10px;width:100%;vertical-align:middle">
                <a href="https://iopscience.iop.org/article/10.1088/1361-665X/ad74bf">
                  <papertitle>Spring-reinforced pneumatic actuator and soft robotic applications</papertitle>
                </a>
                <br>
                <a href="https://scholar.google.com/citations?user=JlilfjsAAAAJ&hl=zh-CN">Boyu Zhang</a>, 
                <strong>Xiangming Gu</strong>,
                Jiayuan Liu, 
                Jingyi Kang, 
                Chengquan Hu,
                <a href="http://at3d.med.tsinghua.edu.cn/en/members/Professor.html">Hongen Liao</a>
                <br>
                Smart Materials and Structures (2024).
                <br>
                <a href="https://iopscience.iop.org/article/10.1088/1361-665X/ad74bf">IOP Science</a>
              </td>
            </tr> -->
  
      <!-- </tbody></table>
      
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
        <td style="padding:10px;width:100%;vertical-align:middle">
          <heading><u>2023</u></heading>

        </td>

      </tr>
    </tbody></table>

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> -->
     
      <!-- <tr>
          <td style="padding:10px;width:100%;vertical-align:middle">
            <a href="https://ieeexplore.ieee.org/document/10262362">
              <papertitle>Disentangled Adversarial Domain Adaptation for Phonation Mode Detection in Singing and Speech</papertitle>
            </a>
            <br>
            <a href="https://cn.linkedin.com/in/yixin-wang-alice">Yixin Wang</a>,
            <a href="https://scholar.google.com/citations?user=4QtVJoEAAAAJ">Wei Wei</a>,
            <strong>Xiangming Gu</strong>,
            <a href="https://ieeexplore.ieee.org/author/37275635600">Xiaohong Guan</a>,
            <a href="https://www.comp.nus.edu.sg/cs/people/wangye/">Ye Wang</a>
            <br>
            IEEE Transactions on Audio, Speech and Language Processing (<em>TASLP</em>'2023).
            <br>
            <a href="https://ieeexplore.ieee.org/document/10262362">IEEE document</a> /
            <a href="https://github.com/aliceyixin/PMD-SingingSpeech">code</a>
          </td>
        </tr> -->

    
          <!-- </tbody></table>
					
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <heading><u>Undergraduate</u></heading>
  
            </td>
  
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> -->

          <!-- <tr>
              <td style="padding:10px;width:100%;vertical-align:middle">
                <a href="https://ieeexplore.ieee.org/document/9798770">
                  <papertitle>Boosting Monocular 3D Human Pose Estimation with Part Aware Attention</papertitle>
                </a>
                <br>
                <a href="https://scholar.google.com/citations?user=NksnxhwAAAAJ&hl=zh-CN">Youze Xue</a>, 
                <a href="https://scholar.google.com/citations?user=A1gA9XIAAAAJ&hl=zh-CN">Jiansheng Chen</a>,  
                <strong>Xiangming Gu</strong>,
                <a href="https://scholar.google.com/citations?user=32hwVLEAAAAJ&hl=en">Huimin Ma</a>,
                <a href="http://web.ee.tsinghua.edu.cn/mahongbing/en/index.htm">Hongbing Ma</a>
                <br>
                 IEEE Transactions on Image Processing (<em>TIP</em>'2022).
                <br>
                <a href="https://ieeexplore.ieee.org/document/9798770">IEEE document</a> /
                <a href="https://github.com/thuxyz19/3D-HPE-PAA">code</a>
              </td>
            </tr> -->
            
          <!-- <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
							<a href="https://ieeexplore.ieee.org/abstract/document/9501981">
                <papertitle>Laser Endoscopic Manipulator Using Spring-Reinforced Multi-DoF Soft Actuator</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=JlilfjsAAAAJ&hl=zh-CN">Boyu Zhang</a>, 
              <a href="https://www.researchgate.net/scientific-contributions/Penghui-Yang-2149548367">Penghui Yang</a>,  
              <strong>Xiangming Gu</strong>,
              <a href="http://at3d.med.tsinghua.edu.cn/en/members/Professor.html">Hongen Liao</a>
              <br>
               IEEE/RSJ International Conference on Intelligent Robots and Systems (<em>IROS</em>'2021), Virtual.
               <br>
               Also in IEEE Robotics and Automation Letter (<em>RA-L</em>'2021).
              <br>
							<a href="https://ieeexplore.ieee.org/abstract/document/9501981">IEEE document</a>
            </td>
          </tr> -->

        <!-- </tbody></table> -->


          <HR>
          
             <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10"><tbody>
              <tr>
                <td>
                  <heading>Open-Sourced Projects</heading>
                </td>
              </tr>
            </tbody></table>
            <table width="100%" align="center" border="0" cellpadding="10"><tbody>
              
              <tr>
                <td width="100%" valign="center">
                  <a href="https://github.com/google-deepmind/gemma_penzai">gemma_penzai</a>: A JAX Research Toolkit for Visualizing, Manipulating, and Understanding <a href="https://github.com/google-deepmind/gemma">Gemma</a> Models with <strong>Multi-modal Support</strong>. Tutorials on <a href="https://github.com/google-deepmind/gemma_penzai/blob/main/notebooks/mech_interp/attention_and_logit_lens.ipynb">attention sink</a>, <a href="https://github.com/google-deepmind/gemma_penzai/blob/main/notebooks/mech_interp/attention_and_logit_lens.ipynb">logit-lens</a>, <a href="https://github.com/google-deepmind/gemma_penzai/blob/main/notebooks/mech_interp/gemma_scope1.ipynb">Gemma Scope 1</a> and <a href="https://github.com/google-deepmind/gemma_penzai/blob/main/notebooks/mech_interp/gemma_scope2.ipynb">2</a>. <a href="https://x.com/gu_xiangming/status/2012886825019609294?s=20">Release post</a>.
                </td>
              </tr>
            </tbody></table>
          <HR>
          
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10"><tbody>
              <tr>
                <td>
                  <heading>Industry Experience</heading>
                </td>
              </tr>
            </tbody></table>
            <table width="100%" align="center" border="0" cellpadding="10"><tbody>
            <tr>
              <td>
                <img id="img-opt" src="images/gdm_logo.png" alt="project_img" width="160" style="border-style: none">
              </td>
  
              <td valign="top" width="70%">
                <papertitle>Google Deepmind
                  <br>
                  Student Researcher</papertitle><br><br>
                  <i><span style="font-size: 9pt;">
                    05.2025 - 10.2025 (London, United Kingdom), 11.2025 - 01.2026 (Singapore) 
                  </i>
                  <br>
                  Hosted by <a href="https://petar-v.com/">Petar Veliƒçkoviƒá</a> and <a href="https://scholar.google.ru/citations?user=jM6Y0yAAAAAJ&hl=en">Larisa Markeeva</a>.
                  <br>
                  Also worked closed with <a href="https://www.razp.info">Razvan Pascanu</a> and <a href="https://sohamde.github.io">Soham De</a>.
                  <br>
                Research on reasoning and test-time-scaling of LLMs. Developing <a href="https://github.com/google-deepmind/gemma_penzai">gemma_penzai</a> to debug LLMs.
                </a> 
              </td>
            </tr>

            <tr>
              <td>
                <img id="img-opt" src="images/sail_logo.png" alt="project_img" width="160" style="border-style: none">
              </td>
  
              <td valign="top" width="70%">
                <papertitle>Sea AI Lab (Sea Limited)
                  <br>
                  Research Intern</papertitle><br><br>
                  <i><span style="font-size: 9pt;">
                    03.2023 - 04.2025 (Singapore) 
                  </i>
                  <br>
                  Mentored by <a href="https://p2333.github.io">Tianyu Pang</a> and <a href="https://duchao0726.github.io">Chao Du</a>.
                  <br>
                  Also worked closed with <a href="https://siviltaram.github.io">Qian Liu</a> and <a href="https://linmin.me">Min Lin</a>.
                  <br>
                Understanding, advancing, and safely deploying generative models and agents.
                </a> 
              </td>
            </tr>


            </table>
             
          <HR>
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10"><tbody>
              <tr>
                <td>
                  <heading>Education</heading>
                </td>
              </tr>
            </tbody></table>
            <table width="100%" align="center" border="0" cellpadding="10"><tbody>

            <tr>
              <td>
                <img id="img-opt" src="images/nus_logo.jpg" alt="project_img" width="160" style="border-style: none">
              </td>
  
              <td valign="top" width="70%">
                <papertitle>National University of Singapore
                  <br>
                  Ph.D. candidate in Computer Science</papertitle><br><br>
                  <i><span style="font-size: 9pt;">
                    08.2021 - 02.2026 (Singapore) 
                  </i>
                  <br>
                  Supervised by Prof. <a href="https://www.comp.nus.edu.sg/cs/people/wangye/">Ye Wang</a>.
                  <br>
                Research on speech, singing and multi-modality.
                </a> 
              </td>
            </tr>

              <tr>
              <td>
                <img id="img-opt" src="images/tsinghua_logo.png" alt="project_img" width="160" style="border-style: none">
              </td>
  
              <td valign="top" width="70%">
                <papertitle>Tsinghua University
                  <br>
                  B.E. degree in Electronic Engineering and B.S. degree in Finance</papertitle><br><br>
                  <i><span style="font-size: 9pt;">
                    08.2017 - 06.2021 (Beijing, China) 
                  </i>
                  <br>
                  Supervised by Prof. <a href="https://scholar.google.com/citations?user=A1gA9XIAAAAJ">Jiansheng Chen</a>.
                  <br>
                Research on computer vision.
                </a>
              </td>
            </tr>

            </table>
             
          <HR>
        
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10"><tbody>
              <tr>
                <td>
                  <heading>Honors and Awards</heading>
                </td>
              </tr>
            </tbody></table>
            <table width="100%" align="center" border="0" cellpadding="10"><tbody>
              
              <tr>
                <td width="100%" valign="center">
                  <!-- Participant for Global Young Scientists Summit, 2025
                  <br> -->
                  Dean's Graduate Research Excellence Award, National University of Singapore, 2024
                  <br>
                  Research Achievement Award, National University of Singapore, 2025/2022
                  <br>
                  MM'22 Top Paper Award, Association for Computing Machinery, 2022
                  <br>
                  President's Graduate Fellowship, National University of Singapore, 2021-2025
                  <br>
                  Tsinghua's Friend- Zheng Geru Scholarship (Academic Excellence Scholarship), Tsinghua University, 2018
                </td>
              </tr>
            </tbody></table>
          <HR>

          
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10"><tbody>
              <tr>
                <td>
                  <heading>Talks and Sharings</heading>
                </td>
              </tr>
            </tbody></table>
            <table width="100%" align="center" border="0" cellpadding="10"><tbody>
              
              <tr>
                <td width="100%" valign="center">
                  <strong>[2026.01]</strong>: <a href="https://aerlabs.tech">AER Labs</a> and <a href="https://ns.com">Network School</a>, invited talk on <a href="./pdf/attention_sink_ns_talk.pdf">Demystifying Attention Sink in LLMs and its Applications to Architecture Design</a>.
                  <br>
                  <strong>[2025.11]</strong>: <a href="https://www.ee.tsinghua.edu.cn/en/">Department of Electronic Engineering, Tsinghua University</a> and <a href="https://hunyuan.tencent.com/visual">Tencent Hunyuan</a>, invited talk on <a href="pdf/attention_sink_hunyuan_talk.pdf">Attention Sink in LLMs and its Applications</a>.
                  <br>
                  <strong>[2025.10]</strong>: Google Deepmind Team DL: Agent Frontier, talk on <strong>Looking into LLMs: From Tokens to Solutions</strong>.
                  <br>
                  <strong>[2025.06]</strong>: Google Deepmind Team DL: Agent Frontier, talk on <strong>Understanding Attention Sink in (Large) Language Models</strong>.
                  <br>
                  <strong>[2025.05]</strong>: <a href="https://asap-seminar.github.io">ASAP Seminar Series</a>, invited talk on <a href="pdf/ASAP_seminar_attention_sink.pdf">When Attention Sink Emerges in Language Models: An Empirical View</a>.
                  <br>
                  <strong>[2025.04]</strong>: <a href="https://far.ai/events/event-list/singapore-aw-25">Singapore Alignment Workshop</a>, poster presentation on <a href="pdf/gyss_agent_smith_poster.pdf">Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast</a>.
                  <br>
                  <strong>[2025.02]</strong>: <a href="https://researchweek.comp.nus.edu.sg/computing/">NUS Research Week Open House</a>, invited talk on <a href="pdf/Research_week_talk.pdf">On the Interpretability and Safety of Generative Models </a>.
                  <br>
                  <strong>[2025.01]</strong>: <a href="https://gyss.nrf.gov.sg">Global Young Scientists Summit</a>, poster presentation on <a href="pdf/gyss_agent_smith_poster.pdf">Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast</a>.
                </td>
              </tr>
            </tbody></table>
          <HR>

        
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10"><tbody>
              <tr>
                <td>
                  <heading>Academic Services</heading>
                </td>
              </tr>
            </tbody></table>
            <table width="100%" align="center" border="0" cellpadding="10"><tbody>
              
              <tr>
                <td width="100%" valign="center">
                  Conference reviewer for NeurIPS, ICML, ICLR, CVPR, ICCV, ECCV, ACL ARR, MM, IJCAI, AISTATS
                  <br>
                  <!-- Workshop reviewer for (i) ICLR 2025 Workshop on Deep Generative Model in Machine Learning: Theory, Principle and Efficacy, (ii) NeurIPS 2024 Workshop on Attributing Model Behavior at Scale, (iii) NeurIPS 2024 Safe Generative AI Workshop
                  <br> -->
                  Journal reviewer for TPAMI, TOMM, TASLP, RA-L
                </td>
              </tr>
              
    
            </tbody></table>
        

        <HR>
        
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10"><tbody>
            <tr>
              <td>
                <heading>Teaching Services</heading>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="10"><tbody>
            
            <tr>
              <td width="100%" valign="center">
                Teaching Assistant, CS4347/CS5647, Sound and Music Computing, Fall 2024
                <br>
                Teaching Assistant, CS6212, Topics in Media, Spring 2024
                <br>
                Teaching Assistant, CS5242, Neural Networks and Deep Learning, Spring 2023
                <br>
                Teaching Assistant, <a href="https://knmnyn.github.io/cs3244-2210">CS3244, Machine Learning, Fall 2022</a>
                <br>
                Teaching Assistant, <a href="https://github.com/xbresson/CS4243_2022">CS4243, Computer Vision and Pattern Recognition, Spring 2022</a>
                <br>
              </td>
            </tr>
            
  
          </tbody></table>

        <HR>
        
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10"><tbody>
            <tr>
              <td>
                <heading>Miscellaneous</heading>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="10"><tbody>
            
            <tr>
              <td width="100%" valign="center">
                I love tourism, movies, food, etc. I have been lived in üá®üá≥üá∏üá¨üá¨üáß, and travelled to üáπüá≠üá´üáÆüáµüáπüáßüá™üá∫üá∏üá≠üá∞üá≤üáæüá®üá¶üá¶üá™üá¶üáπüáØüáµüá≠üá∫üá®üáøüáÆüáπüáªüá¶üá≠üá∑üá´üá∑üá®üá≠üá©üá™üá≥üá±üá∞üá∑ for holidays/conferences.
                <br>
              </td>
            </tr>
            
  
          </tbody></table>

        <HR>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                You've probably seen this website template before, thanks to <a href="https://jonbarron.info">Jon Barron</a>.
                <!-- <br>
                Last Updated July 2024. -->
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
