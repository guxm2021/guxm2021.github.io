<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Xiangming Gu</title>
  
  <meta name="author" content="Xiangming Gu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Xiangming Gu</name>
              </p>
              <p>I am a fourth-year Ph.D. candidate from <a href="https://smcnus.github.io">NUS Sound and Music Computing Lab</a>, where I am supervised by Prof. <a href="https://www.comp.nus.edu.sg/cs/people/wangye/">Ye Wang</a>. I am affiliated to <a href="https://isep.nus.edu.sg">Integrative Sciences and Engineering Programme</a> and <a href="https://www.comp.nus.edu.sg">School of Computing</a> at <a href="https://www.nus.edu.sg">National University of Singapore</a>. Before that, I obtained my B.E. degree of Electronic Engineering and B.S. degree of Finance at <a href="https://www.tsinghua.edu.cn/en/">Tsinghua University</a> in 2021.
              </p>
              <p>
                I am also a research intern at <a href="https://sail.sea.com">Sea AI lab (SAIL)</a>. I am mentored by Dr. <a href="https://p2333.github.io">Tianyu Pang</a> and Dr. <a href="https://duchao0726.github.io">Chao Du</a>, and working closely with Dr. <a href="https://siviltaram.github.io">Qian Liu</a>, and Dr. <a href="https://linmin.me">Min Lin</a>.
              </p>
              <p>
                My research topics include: (i) <strong>generative models</strong>: demystifying attention sink in LLMs [<a href="https://arxiv.org/abs/2410.10781">ICLR'2025</a>], and demystifying memorization in diffusion models [<a href="https://arxiv.org/pdf/2310.02664">TMLR'2025</a>]; (ii) <strong>AI safety</strong>: introducing infectious jailbreak for (multimodal) LLM multi-agents [<a href="https://openreview.net/pdf/5e3d2c9612d18846d22f7e5dc0568925afb9f6ca.pdf">ICML'2024</a>], and elucidating confidence calibration on LLM guard models [<a href="https://arxiv.org/abs/2410.10414">ICLR'2025</a>]; (iii) <strong>singing/speech</strong>: application of machine learning, e.g. multimodal learning [<a href="https://dl.acm.org/doi/pdf/10.1145/3503161.3548411">MM'2022 Oral</a>, <a href="https://dl.acm.org/doi/pdf/10.1145/3651310">TOMM'2024</a>], multi-distribution learning (domain adaptation [<a href="https://arxiv.org/pdf/2207.09747">ISMIR'2022</a>, <a href="https://dl.acm.org/doi/pdf/10.1145/3651310">TOMM'2024</a>], fairness [<a href="https://dl.acm.org/doi/pdf/10.1145/3581783.3612272">MM'2023</a>]), to singing/speech techniques.
              </p>
              <p>
                <font color="red"><strong>I am looking for full-time research scientist positions, please contact me if you are interested in my research.</strong></font>
              </p>

              <p style="text-align:center">
                <a href="mailto:xiangming@u.nus.edu">Email</a> &nbsp/&nbsp
                <a href="pdf/Xiangming_Gu_Resume.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=BkxEuIoAAAAJ&hl">Google Scholar</a> &nbsp/&nbsp
                <a href="https://openreview.net/profile?id=~Xiangming_Gu1">Openreview</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/xiangming-gu/">Linkedin</a> &nbsp/&nbsp
                <a href="https://twitter.com/gu_xiangming">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/guxm2021">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/XiangmingGu.JPG"><img style="width:75%;max-width:75%" alt="profile photo" src="images/XiangmingGu.JPG" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        
        <HR>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:10px;width:100%;vertical-align:middle">
            <heading>News</heading>

          <ul>
            <li>
              <strong>[2025.01]</strong>: Two papers with one spotlight and one poster got accepted to International Conference on Learning Representations (<i>ICLR</i>'2025), one paper got accepted to Transactions on Machine Learning Research (<i>TMLR</i>'2025)!
            </li>
            <li>
              <strong>[2025.01]</strong>: I made a <a href="pdf/gyss_agent_smith_poster.pdf">poster presentation</a> about Agent Smith during Global Young Scientists Summit 2025!
            </li>
            <li>
              <strong>[2024.10]</strong>: I was selected as a participant for Global Young Scientists Summit 2025!
            </li>
            <li>
              <strong>[2024.07]</strong>: I received Dean's Graduate Research Excellence Award from School of Computing, NUS!
            </li>
            <li>
              <strong>[2024.05]</strong>: One paper got accepted to International Conference on Machine Learning (<i>ICML</i>'2024)!
            </li>
            <li>
              <strong>[2024.02]</strong>: One paper got accepted to ACM Transactions on Multimedia Computing Communications and Applications (<i>TOMM</i>'2024)!
            </li>
            <li>
              <strong>[2024.02]</strong>: We released <a href="https://arxiv.org/abs/2402.08567">Agent Smith</a>, which got posted as "Here Come the AI Worms" on <a href="https://www.wired.com/story/here-come-the-ai-worms/?utm_social-type=owned&utm_source=linkedin&utm_medium=social&utm_campaign=wired&utm_brand=wired&mbid=social_linkedin">WIRED Magazine</a>!
          </li>
            <!-- <li>
              <strong>[2023.09]</strong>: One paper got accepted to IEEE Transactions on Audio, Speech and Language Processing (<i>TASLP</i>'2023)!
          </li>
            <li>
              <strong>[2023.07]</strong>: One paper got accepted to ACM International Conference on Multimedia (<i>MM</i>'2023)!
          </li> -->
            <!-- <li>
              <strong>[2023.03]</strong>: I joined <a href="https://sail.sea.com">Sea AI lab (SAIL)</a> as a research intern and my research is related to generative models and (multimodal) large language models!
          </li> -->
            <!-- <li>
              <strong>[2023.03]</strong>: I was invited to be a reviewer for International Conference on Computer Vision (<i>ICCV</i>'2023)!
          </li> -->
            <!-- <li>
              <strong>[2023.01]</strong>: I received Research Achievement Award from School of Computing, NUS!
          </li>
          <li>
              <strong>[2022.12]</strong>: One paper got accepted to Transactions on Machine Learning Research (<i>TMLR</i>'2022)!
          </li>
          <li>
              <strong>[2022.12]</strong>: I passed my Ph.D. Qualifying Examination (PQE) and became a Ph.D. candidate!
          </li> -->
          <!-- <li>
            <strong>[2022.11]</strong>: I physically attended <i>NeurIPS</i>'2022 in New Orleans, United States!
          </li>
          <li>
            <strong>[2022.10]</strong>: I physically attended <i>MM</i>'2022 in Lisbon, Portugal, where our paper <a href="https://dl.acm.org/doi/10.1145/3503161.3548411">MM-ALT</a> won the <strong>Top Paper Award</strong>!
          </li> -->
            <!-- <li>
              <strong>[2022.09]</strong>: One paper got accepted to Advances in Neural Information Processing Systems (<i>NeurIPS</i>'2022)!
          </li>
            <li>
              <strong>[2022.07]</strong>: One paper got accepted to International Society for Music Information Retrieval Conference (<i>ISMIR</i>'2022)!
          </li>
            <li>
                <strong>[2022.06]</strong>: One paper got accepted to ACM International Conference on Multimedia (<i>MM</i>'2022) as oral presentation, which also won the <strong>Top Paper Award</strong>!
            </li>
            <li>
              <strong>[2022.05]</strong>: One paper got accepted to IEEE Transactions on Image Processing (<i>TIP</i>'2022)!
          </li> -->
          </ul>
          </td>

        </tr>
      </tbody></table>

      <HR>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:10px;width:100%;vertical-align:middle">
            <heading>Selected Research</heading>
          </td>
        </tr>
      </tbody></table>
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <!-- <table width="100%" align="center" border="0" cellpadding="10"><tbody> -->
              
          <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              * denotes equal contribution, &dagger; denotes correspondence. Please see my <a href="pdf/Xiangming_Gu_Resume.pdf">CV</a> or
              <a href="https://scholar.google.com/citations?user=BkxEuIoAAAAJ&hl">Google Scholar</a> for full list.
            </td>
          </tr>
          

        <!-- </tbody></table> -->
      
        <tr>
          <td style="padding:10px;width:100%;vertical-align:middle">
            <u>Generative Models</u>
          </td>
  
        </tr>

        <tr>
          <td style="padding:10px;width:100%;vertical-align:middle">
            <a href="https://openreview.net/forum?id=78Nn4QJTEN">
              <papertitle>When Attention Sink Emerges in Language Models: An Empirical View</papertitle>
            </a>
            <br>
            <u><strong>Xiangming Gu</strong></u>,
            <a href="https://p2333.github.io">Tianyu Pang</a>&dagger;,
            <a href="https://duchao0726.github.io">Chao Du</a>,
            <a href="https://siviltaram.github.io">Qian Liu</a>,
            <a href="https://openreview.net/profile?id=~Fengzhuo_Zhang1">Fengzhuo Zhang</a>,
            <a href="https://scholar.google.com.hk/citations?user=4gFE1iYAAAAJ&hl">Cunxiao Du</a>,
            <a href="https://www.comp.nus.edu.sg/cs/people/wangye/">Ye Wang</a>&dagger;,
            <a href="http://linmin.me">Min Lin</a>
            <br>
              International Conference on Learning Representations (<strong>ICLR</strong>), Singapore, Singapore, 2025. (<font color="red"><strong>Spotlight</strong></font>)
            <br>
            Also in Annual Conference on Neural Information Processing Systems Workshop on Attributing Model Behavior at Scale (<strong>ATTRIB @ NeurIPS</strong>), Vancouver, Canada, 2024. (<font color="red"><strong>Oral</strong></font>)
            <br>
            <a href="https://arxiv.org/pdf/2410.10781">pdf</a> /
            <a href="https://github.com/sail-sg/Attention-Sink">code</a>
            <!-- <p></p> -->
          </td>
        </tr>
        
          <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2310.02664">
                <papertitle>On Memorization in Diffusion Models</papertitle>
              </a>
              <br>
              <u><strong>Xiangming Gu</strong></u>,
              <a href="https://duchao0726.github.io">Chao Du</a>&dagger;,
              <a href="https://p2333.github.io">Tianyu Pang</a>&dagger;,
              <a href="https://zhenxuan00.github.io">Chongxuan Li</a>,
              <a href="http://linmin.me">Min Lin</a>,
              <a href="https://www.comp.nus.edu.sg/cs/people/wangye/">Ye Wang</a>&dagger;
              <br>
              Transactions on Machine Learning Research (<strong>TMLR</strong>), 2025.
              <br>
              <a href="https://arxiv.org/pdf/2310.02664.pdf">pdf</a> /
              <a href="https://github.com/sail-sg/DiffMemorize">code</a>		
              <!-- <p>We first show that the theoretical optimum of denoising score matching can only replicate training data, which contradicts the generalization ability of state-of-the-art diffusion models. Then we explore the conditions under which trained diffusion models will demonstrate the similar memorization behaviors as well as the influential factors from three facets of data distribution, model configuration, and training procedure. Intriguingly, we find that conditioning training data on uninformative random labels can significantly trigger the memorization in diffusion models.</p> -->
              <!-- <p></p> -->
            </td>
          </tr>
          
          <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <u>AI Safety</u>
            </td>
    
          </tr>

          <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <a href="https://openreview.net/forum?id=DYMj03Gbri">
                <papertitle>Agent Smith: A Single Image Can Jailbreak <i>One Million</i> Multimodal LLM Agents Exponentially Fast</papertitle>
              </a>
              <br>
              <u><strong>Xiangming Gu</strong></u>*,
              <a href="https://xszheng2020.github.io">Xiaosen Zheng</a>*,
              <a href="https://p2333.github.io">Tianyu Pang</a>*&dagger;,
              <a href="https://duchao0726.github.io">Chao Du</a>,
              <a href="https://siviltaram.github.io">Qian Liu</a>,
              <a href="https://www.comp.nus.edu.sg/cs/people/wangye/">Ye Wang</a>&dagger;,
              <a href="http://www.mysmu.edu/faculty/jingjiang/">Jing Jiang</a>&dagger;,
              <a href="http://linmin.me">Min Lin</a>
              <br>
              International Conference on Machine Learning (<strong>ICML</strong>), Vienna, Austria, 2024.
              <br>
              Also in International Conference on Learning Representations Workshop on Large Language Model Agents (<strong>LLMAgents @  ICLR</strong>), Vienna, Austria, 2024.
              <br>
              <a href="pdf/ICML2024.pdf">pdf</a> /
              <a href="https://sail-sg.github.io/Agent-Smith/">project page</a> /
              <a href="https://github.com/sail-sg/Agent-Smith">code</a> /
              <a href="https://youtu.be/_UHudWD9lmY">video</a> /
              <a href="pdf/ICML2024_slides.pdf">slides</a> /
              <a href="pdf/ICML2024_poster.pdf">ICML poster</a> /
              <a href="pdf/gyss_agent_smith_poster.pdf">GYSS poster</a> /
              <a href="https://www.wired.com/story/here-come-the-ai-worms/?utm_social-type=owned&utm_source=linkedin&utm_medium=social&utm_campaign=wired&utm_brand=wired&mbid=social_linkedin">press</a>
              <!-- <p>We present <i>infectious jailbreak</i>, which entails the adversary simply jailbreaking a single agent, and without any further intervention from the adversary, (almost) all agents will become infected exponentially fast and exhibit harmful behaviors. Our results show that feeding an (infectious) adversarial image into the memory of any randomly chosen agent is sufficient to achieve infectious jailbreak in multi-agent environments containing up to <i>one million</i> LLaVA-1.5 agents. Finally, we derive a simple principle for determining whether a defense mechanism can provably restrain the spread of infectious jailbreak.</p> -->
              <!-- <p></p> -->
            </td>
          </tr>

          <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <a href="https://openreview.net/forum?id=wUbum0nd9N">
                <papertitle>On Calibration of LLM-based Guard Models for Reliable Content Moderation</papertitle>
              </a>
              <br>
              <a href="https://waffle-liu.github.io">Hongfu Liu</a>&dagger;,
              <a href="https://sites.google.com/view/huanghengguan/homepage">Hengguan Huang</a>,
              <a href="http://www.wanghao.in">Hao Wang</a>,
              <u><strong>Xiangming Gu</strong></u>,
              <a href="https://www.comp.nus.edu.sg/cs/people/wangye/">Ye Wang</a>
              <br>
              International Conference on Learning Representations (<strong>ICLR</strong>), Singapore, Singapore, 2025.
              <br>
              Also in Annual Conference on Neural Information Processing Systems Safe Generative AI Workshop (<strong>SafeGenAI @ NeurIPS</strong>), Vancouver, Canada, 2024. (<font color="red"><strong>Oral</strong></font>)
              <br>
              <a href="https://arxiv.org/pdf/2410.10414">pdf</a> /
              <a href="https://github.com/Waffle-Liu/calibration_guard_model">code
              <!-- <p></p> -->
            </td>
          </tr>

          <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <u>Speech and Singing</u>
            </td>
    
          </tr>

            <tr>
              <!-- <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/arxiv2022.png" width="160" height="160">
              </td> -->
                <td style="padding:10px;width:100%;vertical-align:middle">
                  <a href="https://dl.acm.org/doi/10.1145/3651310">
                    <papertitle>Automatic Lyric Transcription and Automatic Music Transcription from Multimodal Singing</papertitle>
                  </a>
                  <br>
                  <u><strong>Xiangming Gu</strong></u>,
                  <a href="https://www.oulongshen.xyz">Longshen Ou</a>,
                  <a href="https://scholar.google.com/citations?hl=zh-CN&user=Ffq7lrcAAAAJ">Wei Zeng</a>,
                  <a href="https://www.linkedin.com/in/jianan-zhang-a9014a173/?originalSubdomain=cn">Jianan Zhang</a>,
                  <a href="https://nic-wong.carrd.co">Nicholas Wong</a>,
                  <a href="https://www.comp.nus.edu.sg/cs/people/wangye/">Ye Wang</a>&dagger;
                  <br>
                  ACM Transactions on Multimedia Computing Communications and Applications (<strong>TOMM</strong>), 2024.
                  <br>
                  <a href="https://dl.acm.org/doi/pdf/10.1145/3651310">pdf</a> /
                  <a href="https://github.com/guxm2021/SVT_SpeechBrain">code</a> /
                  <a href="https://zenodo.org/records/10814703">data</a>
                  <!-- <p></p> -->
                </td>
              </tr>

              <tr>
                <!-- <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/arxiv2022.png" width="160" height="160">
                </td> -->
                  <td style="padding:10px;width:100%;vertical-align:middle">
                    <a href="https://dl.acm.org/doi/10.1145/3581783.3612272">
                      <papertitle>Elucidate Gender Fairness in Singing Voice Transcription</papertitle>
                    </a>
                    <br>
                    <u><strong>Xiangming Gu</strong></u>,
                    <a href="https://scholar.google.com/citations?hl=zh-CN&user=Ffq7lrcAAAAJ">Wei Zeng</a>,
                    <a href="https://www.comp.nus.edu.sg/cs/people/wangye/">Ye Wang</a>&dagger;
                    <br>
                    ACM International Conference on Multimedia (<strong>MM</strong>), Ottawa, Canada, 2023.
                    <br>
                    <a href="pdf/ACMMM2023.pdf">pdf</a> /
                    <a href="https://github.com/guxm2021/SVT_SpeechBrain">code</a> /
                    <a href="https://youtu.be/lMQE7hQlTCQ">video</a> /
                    <a href="pdf/ACMMM2023_poster.pdf">poster</a>
                    <!-- <p></p> -->
                    <!-- <p>We first attempted the fairness topic within the singing-centric deep learning community. We presented evidence that Singing Voice Transcription (SVT) systems demonstrate female superiority. To tackle this, we proposed a note-conditioned adversarial learning approach to mitigate this gender bias in SVT and offer better fairness-utility trade-offs.</p> -->
                  </td>
                </tr>

                <tr>
                  <!-- <td style="padding:20px;width:25%;vertical-align:middle">
                    <img src="images/acmmm2022.png" width="160" height="160">
                  </td> -->
                    <td style="padding:10px;width:100%;vertical-align:middle">
                      <a href="https://dl.acm.org/doi/10.1145/3503161.3548411">
                        <papertitle>MM-ALT: A Multimodal Automatic Lyric Transcription System</papertitle>
                      </a>
                      <br>
                      <u><strong>Xiangming Gu</strong></u>*,
                      <a href="https://www.oulongshen.xyz">Longshen Ou</a>*,
                      <a href="https://www.linkedin.com/in/danielle-ong-854b88177/">Danielle Ong</a>,
                      <a href="https://www.comp.nus.edu.sg/cs/people/wangye/">Ye Wang</a>&dagger;
                      <br>
                      ACM International Conference on Multimedia (<strong>MM</strong>), Lisbon, Portugal, 2022. (<font color="red"><strong>Oral, Top Paper Award</strong></font>)
                      <br>
                      <a href="pdf/ACMMM2022.pdf">pdf</a> /
                      <a href="pdf/ACMMM2022_Appendix.pdf">appendix</a> /
                      <!-- <a href="data/gu2022mm.bib">bibtex</a> / -->
                      <a href="https://n20em.github.io">project page</a> /
                      <a href="https://github.com/guxm2021/MM_ALT">code</a> /
                      <a href="https://zenodo.org/records/7545968">data</a> /
                      <a href="https://youtu.be/3eA62RYROq0">video</a> /
                      <a href="https://www.comp.nus.edu.sg/news/features/2023-marvellous-richness-wye/">press</a>
                      <!-- <p></p> -->
                      <!-- <p>We proposed the problem setting of multimodal automatic lyric transcription (ALT) as well as its solution: MM-ALT which transcribes the lyrics from synchronized audio, video and IMU modalities. Our experiments demonstrated MM-ALT is more robust to noise perturbations than its single-modal counterparts. We also curated the first multimodal ALT dataset: N20EM.</p> -->
                    </td>
                  </tr>


                  <tr>
                    <!-- <td style="padding:20px;width:25%;vertical-align:middle">
                      <img src="images/ismir2022.png" width="160" height="160">
                    </td> -->
                      <td style="padding:10px;width:100%;vertical-align:middle">
                        <a href="https://arxiv.org/abs/2207.09747">
                          <papertitle>Transfer Learning of wav2vec 2.0 for Automatic Lyric Transcription</papertitle>
                        </a>
                        <br>
                        <a href="https://www.oulongshen.xyz">Longshen Ou</a>*,
                        <u><strong>Xiangming Gu</strong></u>*,
                        <a href="https://www.comp.nus.edu.sg/cs/people/wangye/">Ye Wang</a>&dagger;
                        <br>
                        International Society for Music Information Retrieval Conference (<strong>ISMIR</strong>), Bengaluru, India, 2022.
                        <br>
                        <a href="pdf/ISMIR2022.pdf">pdf</a> /
                        <!-- <a href="data/ou2022towards.bib">bibtex</a> / -->
                        <a href="https://github.com/guxm2021/ALT_SpeechBrain">code</a>
                        <!-- <p></p> -->
                        <!-- <p>We proposed to transfer the self-supervised-learning models, e.g. wav2vec 2.0, from the speech domain to the singing domain. Our automatic lyric transcription (ALT) system achieved state-of-the-art performances on various benchmark singing datasets.</p> -->
                      </td>
                    </tr>

        <!-- <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2304.12082.pdf">
                <papertitle>Deep Audio-Visual Singing Voice Transcription based on Self-Supervised Learning Models</papertitle>
              </a>
              <br>
              <strong>Xiangming Gu</strong>,
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=Ffq7lrcAAAAJ">Wei Zeng</a>,
              <a href="https://www.linkedin.com/in/jianan-zhang-a9014a173/?originalSubdomain=cn">Jianan Zhang</a>,
              <a href="https://www.oulongshen.xyz">Longshen Ou</a>,
              <a href="https://www.comp.nus.edu.sg/cs/people/wangye/">Ye Wang</a>&dagger;
              <br>
              <em>Preprints</em>, 2023.
              <br>
              <a href="https://arxiv.org/pdf/2304.12082.pdf">pdf</a> /
              <a href="https://github.com/guxm2021/SVT_SpeechBrain">code</a>		
              <p></p>
              <p>We proposed the first audio-visual singing voice transcription (SVT) system to tackle the sound contamination and extend our N20EM dataset to N20EMv2. Our proposed self-supervised-learning models based SVT system demonstrates state-of-the-art performances on multiple benchmark SVT datasets.</p>
            </td>
          </tr> -->

          <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <u>Bayesian Deep Learning</u>
            </td>
    
          </tr>

          <tr>
            <!-- <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/neurips2022.png" width="160" height="137">
            </td> -->
              <td style="padding:10px;width:100%;vertical-align:middle">
                <a href="https://openreview.net/forum?id=wiHzQWwg3l">
                  <papertitle>Extrapolative Continuous-time Bayesian Neural Network for Fast Training-free Test-time Adaptation</papertitle>
                </a>
                <br>
                <a href="https://sites.google.com/view/huanghengguan/homepage">Hengguan Huang</a>&dagger;,
                <u><strong>Xiangming Gu</strong></u>,
                <a href="http://www.wanghao.in">Hao Wang</a>,
                <a href="https://smcnus.github.io">Chang Xiao</a>,
                <a href="https://waffle-liu.github.io">Hongfu Liu</a>,
                <a href="https://www.comp.nus.edu.sg/cs/people/wangye/">Ye Wang</a>&dagger;
                <br>
                Annual Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>), New Orleans, USA, 2022.
                <br>
                <a href="pdf/NeurIPS2022.pdf">pdf</a> /
                <!-- <a href="data/huang2022.bib">bibtex</a> / -->
                <a href="https://github.com/guxm2021/ECBNN">code /
                <a href="https://recorder-v3.slideslive.com/?share=76177&s=cd597422-3109-42f3-9072-dfef5c351a4d">video</a>
                <!-- <p></p> -->
                <!-- <p>We incorporated the mechanism of internel preditive modeling into unsupervised domain adaptation (UDA) approaches to handle non-stationary streaming data and achieve training-free test-time adaptation with low latency. Our extrapolative continuous-time Bayesian neural network (ECBNN) exceeds UDA baselines on both synthetic and real-world data.</p> -->
              </td>
            </tr>

            <tr>
              <!-- <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/arxiv2022.png" width="160" height="160">
              </td> -->
                <td style="padding:10px;width:100%;vertical-align:middle">
                  <a href="https://openreview.net/forum?id=29V0xo7jKp">
                    <papertitle>Unsupervised Mismatch Localization in Cross-Modal Sequential Data with Application to Mispronunciations Localization</papertitle>
                  </a>
                  <br>
                  <a href="https://scholar.google.com/citations?user=4QtVJoEAAAAJ">Wei Wei</a>*,
                  <a href="https://sites.google.com/view/huanghengguan/homepage">Hengguan Huang</a>*,
                  <u><strong>Xiangming Gu</strong></u>,
                  <a href="http://www.wanghao.in">Hao Wang</a>,
                  <a href="https://www.comp.nus.edu.sg/cs/people/wangye/">Ye Wang</a>&dagger;
                  <br>
                  Transactions on Machine Learning Research (<strong>TMLR</strong>), 2022.
                  <br>
                  <a href="pdf/TMLR2022.pdf">pdf</a> /
                  <!-- <a href="data/wei2022unsupervised.bib">bibtex</a> / -->
                  <a href="https://github.com/weiwei-ww/ML-VAE">code</a>
                  <!-- <p></p> -->
                  <!-- <p>We proposed mismatch localization variational autoencoder (ML-VAE) as well as a novel and effective training procedure, which is an unsupervised learning algorithm, to infer the relationship between content-mismatched cross-modal sequential data, especially for speech-text sequences.</p> -->
                </td>
              </tr>

        </tbody></table>

        <!-- <HR>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <heading>Full Publications</heading>

            </td>

          </tr>
        </tbody></table> -->
        
<!-- 
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:10px;width:100%;vertical-align:middle">
            <heading><u>2024</u></heading>
  
          </td>
  
        </tr>
      </tbody></table> -->



      <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        

          <tr>
              <td style="padding:10px;width:100%;vertical-align:middle">
                <a href="https://iopscience.iop.org/article/10.1088/1361-665X/ad74bf">
                  <papertitle>Spring-reinforced pneumatic actuator and soft robotic applications</papertitle>
                </a>
                <br>
                <a href="https://scholar.google.com/citations?user=JlilfjsAAAAJ&hl=zh-CN">Boyu Zhang</a>, 
                <strong>Xiangming Gu</strong>,
                Jiayuan Liu, 
                Jingyi Kang, 
                Chengquan Hu,
                <a href="http://at3d.med.tsinghua.edu.cn/en/members/Professor.html">Hongen Liao</a>
                <br>
                Smart Materials and Structures (2024).
                <br>
                <a href="https://iopscience.iop.org/article/10.1088/1361-665X/ad74bf">IOP Science</a>
              </td>
            </tr> -->
  
      <!-- </tbody></table>
      
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
        <td style="padding:10px;width:100%;vertical-align:middle">
          <heading><u>2023</u></heading>

        </td>

      </tr>
    </tbody></table>

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> -->
     
      <!-- <tr>
          <td style="padding:10px;width:100%;vertical-align:middle">
            <a href="https://ieeexplore.ieee.org/document/10262362">
              <papertitle>Disentangled Adversarial Domain Adaptation for Phonation Mode Detection in Singing and Speech</papertitle>
            </a>
            <br>
            <a href="https://cn.linkedin.com/in/yixin-wang-alice">Yixin Wang</a>,
            <a href="https://scholar.google.com/citations?user=4QtVJoEAAAAJ">Wei Wei</a>,
            <strong>Xiangming Gu</strong>,
            <a href="https://ieeexplore.ieee.org/author/37275635600">Xiaohong Guan</a>,
            <a href="https://www.comp.nus.edu.sg/cs/people/wangye/">Ye Wang</a>&dagger;
            <br>
            IEEE Transactions on Audio, Speech and Language Processing (<em>TASLP</em>'2023).
            <br>
            <a href="https://ieeexplore.ieee.org/document/10262362">IEEE document</a> /
            <a href="https://github.com/aliceyixin/PMD-SingingSpeech">code</a>
          </td>
        </tr> -->

    
          <!-- </tbody></table>
					
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <heading><u>Undergraduate</u></heading>
  
            </td>
  
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> -->

          <!-- <tr>
              <td style="padding:10px;width:100%;vertical-align:middle">
                <a href="https://ieeexplore.ieee.org/document/9798770">
                  <papertitle>Boosting Monocular 3D Human Pose Estimation with Part Aware Attention</papertitle>
                </a>
                <br>
                <a href="https://scholar.google.com/citations?user=NksnxhwAAAAJ&hl=zh-CN">Youze Xue</a>, 
                <a href="https://scholar.google.com/citations?user=A1gA9XIAAAAJ&hl=zh-CN">Jiansheng Chen</a>&dagger;,  
                <strong>Xiangming Gu</strong>,
                <a href="https://scholar.google.com/citations?user=32hwVLEAAAAJ&hl=en">Huimin Ma</a>,
                <a href="http://web.ee.tsinghua.edu.cn/mahongbing/en/index.htm">Hongbing Ma</a>
                <br>
                 IEEE Transactions on Image Processing (<em>TIP</em>'2022).
                <br>
                <a href="https://ieeexplore.ieee.org/document/9798770">IEEE document</a> /
                <a href="https://github.com/thuxyz19/3D-HPE-PAA">code</a>
              </td>
            </tr> -->
            
          <!-- <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
							<a href="https://ieeexplore.ieee.org/abstract/document/9501981">
                <papertitle>Laser Endoscopic Manipulator Using Spring-Reinforced Multi-DoF Soft Actuator</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=JlilfjsAAAAJ&hl=zh-CN">Boyu Zhang</a>, 
              <a href="https://www.researchgate.net/scientific-contributions/Penghui-Yang-2149548367">Penghui Yang</a>,  
              <strong>Xiangming Gu</strong>,
              <a href="http://at3d.med.tsinghua.edu.cn/en/members/Professor.html">Hongen Liao</a>&dagger;
              <br>
               IEEE/RSJ International Conference on Intelligent Robots and Systems (<em>IROS</em>'2021), Virtual.
               <br>
               Also in IEEE Robotics and Automation Letter (<em>RA-L</em>'2021).
              <br>
							<a href="https://ieeexplore.ieee.org/abstract/document/9501981">IEEE document</a>
            </td>
          </tr> -->

        <!-- </tbody></table> -->


        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading><u>2020</u></heading>
  
              </td>
  
            </tr>
          </tbody></table> -->

          <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  
            <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <a href="https://arxiv.org/pdf/2010.04974.pdf">
                    <papertitle>Distilling a Deep Neural Network into a Takagi-Sugeno-Kang Fuzzy Inference System</papertitle>
                  </a>
                  <br>
                  <strong>Xiangming Gu</strong>,
                  <a href="https://online.ece.nus.edu.sg/staff/web.asp?id=elexc">Xiang Cheng</a>
                  <br>
                  <em>Technical Report</em>, 2020.
                  <br>
                  <a href="pdf/Arxiv2020.pdf">pdf</a>
                  <p></p>
                  <p>We proposed to distill the knowledge from deep learning models into TSK-type Fussy Inference systems, which improves the performance of interpretable models.</p>
                </td>
              </tr>
  
          </tbody></table> -->


          <HR>

        
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10"><tbody>
              <tr>
                <td>
                  <heading>Honors and Awards</heading>
                </td>
              </tr>
            </tbody></table>
            <table width="100%" align="center" border="0" cellpadding="10"><tbody>
              
              <tr>
                <td width="100%" valign="center">
                  Participant for Global Young Scientists Summit, 2025
                  <br>
                  Dean's Graduate Research Excellence Award, National University of Singapore, 2024
                  <br>
                  Research Incentive Award, National University of Singapore, 2023
                  <br>
                  Research Achievement Award, National University of Singapore, 2022
                  <br>
                  MM'22 Top Paper Award, Association for Computing Machinery, 2022
                  <br>
                  MM'22 Student Travel Grant, Association for Computing Machinery, 2022
                  <br>
                  President's Graduate Fellowship, National University of Singapore, 2021
                  <br>
                  Visiting Undergraduate Student Scholarship, Tsinghua University, 2020
                  <br>
                  Tsinghua's Friend- Zheng Geru Scholarship, Tsinghua University, 2018
                </td>
              </tr>
            </tbody></table>
          <HR>

        
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10"><tbody>
              <tr>
                <td>
                  <heading>Academic Services</heading>
                </td>
              </tr>
            </tbody></table>
            <table width="100%" align="center" border="0" cellpadding="10"><tbody>
              
              <tr>
                <td width="100%" valign="center">
                  Conference reviewer for NeurIPS 2024, ICML 2025, ICLR 2025, CVPR 2025, ICCV 2025/2023, ECCV 2024, ACL ARR 2025/2024, MM 2024, IJCAI 2024, AISTATS 2025/2021
                  <br>
                  Workshop reviewer for (i) ICLR 2025 Workshop on Deep Generative Model in Machine Learning: Theory, Principle and Efficacy, (ii) NeurIPS 2024 Workshop on Attributing Model Behavior at Scale, (iii) NeurIPS 2024 Safe Generative AI Workshop
                  <br>
                  Journal reviewer for TOMM, TASLP, RA-L
                </td>
              </tr>
              
    
            </tbody></table>
        

        <HR>
        
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="10"><tbody>
            <tr>
              <td>
                <heading>Teaching</heading>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="10"><tbody>
            
            <tr>
              <td width="100%" valign="center">
                Teaching Assistant, CS4347/CS5647, Sound and Music Computing, Fall 2024
                <br>
                Teaching Assistant, CS6212, Topics in Media, Spring 2024
                <br>
                Teaching Assistant, CS5242, Neural Networks and Deep Learning, Spring 2023
                <br>
                Teaching Assistant, <a href="https://knmnyn.github.io/cs3244-2210">CS3244, Machine Learning, Fall 2022</a>
                <br>
                Teaching Assistant, <a href="https://github.com/xbresson/CS4243_2022">CS4243, Computer Vision and Pattern Recognition, Spring 2022</a>
                <br>
              </td>
            </tr>
            
  
          </tbody></table>

        <HR>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                You've probably seen this website template before, thanks to <a href="https://jonbarron.info">Jon Barron</a>.
                <!-- <br>
                Last Updated July 2024. -->
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
